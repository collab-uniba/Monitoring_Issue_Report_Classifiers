{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Bug and Enhancement Reports with RoBERTa\n",
    "\n",
    "This notebook demonstrates how to train a RoBERTa model for bug and enhancement report classification using the Hugging Face `transformers` framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install torch torchvision\n",
    "%pip install transformers\n",
    "%pip install scikit-learn\n",
    "%pip install accelerate\n",
    "%pip install imbalanced-learn\n",
    "%pip install sentence-transformers\n",
    "%pip install alibi-detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Let's start by importing all the libraries needed for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from alibi_detect.cd import KSDrift\n",
    "from alibi_detect.cd.tensorflow import UAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of Dataset and Parameters\n",
    "Write the parameters in the config.yaml file.\n",
    "\n",
    "Enabling undersampling is recommended if the dataset is unbalanced for Bugs and Enhancements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './config.yaml'\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "dataset_path = config['dataset_path']\n",
    "start_year_train = config['start_year_train']\n",
    "end_year_train = config['end_year_train']\n",
    "last_year_test = config['last_year_test']\n",
    "undersampling_flag = config['undersampling_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the dataset name from the dataset_path\n",
    "dataset_name = os.path.basename(dataset_path).split('.')[0]\n",
    "\n",
    "# Defining the results and model save paths using the dataset name\n",
    "results_path = f'./RESULTS/{dataset_name}'\n",
    "\n",
    "model_save_path = os.path.join(results_path, \"model\")\n",
    "\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "    \n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We load the data from the CSV file and prepare it for training and evaluation. \n",
    "We filter the data to include only those between 2000 and 2007 and correctly label them as Bug (0) or Enhancement (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_all = pd.read_csv(dataset_path)\n",
    "df_all['date'] = pd.to_datetime(df_all['date'], errors='coerce', format='%Y-%m-%dT%H:%M:%S.%f+0000')\n",
    "df_all = df_all[df_all['label'].isin(['Bug', 'Enhancement'])]\n",
    "\n",
    "# Filter data for training and validation (start_year_train - end_year_train)\n",
    "df_train_val = df_all[(df_all['date'].dt.year >= start_year_train) & (df_all['date'].dt.year <= end_year_train)]\n",
    "df_train_val['text'] = df_train_val['title'] + \" \" + df_train_val['body']\n",
    "df_train_val['labels'] = df_train_val['label'].apply(lambda x: 1 if x == 'Enhancement' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Division in Train e Validation\n",
    "We split the data in train (70%) and validation (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df = train_test_split(df_train_val, test_size=0.3, random_state=42, stratify=df_train_val['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling\n",
    "\n",
    "This code initializes a RandomUnderSampler, fits it to the training data, and creates a new dataframe (train_df_resampled) with the undersampled data.\n",
    "You can then use train_df_resampled to train the model instead of train_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersampling_flag:\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "    # Initialize the RandomUnderSampler\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "    # Resample the training data\n",
    "    x_train_resampled, y_train_resampled = rus.fit_resample(train_df.drop('labels', axis=1), train_df['labels'])\n",
    "\n",
    "    # Reconstruct the training dataframe with the resampled data\n",
    "    train_df = pd.concat([x_train_resampled, y_train_resampled], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the Dataset\n",
    "\n",
    "We define a `CustomDataset` class to prepare the data for training with BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_token_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        if pd.isna(text):\n",
    "            text = \"\"  # Replaces NaN values â€‹â€‹with empty strings\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We use the BERT tokenizer to convert text into tokens that the model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_dataset = CustomDataset(train_df['text'].to_numpy(), train_df['labels'].to_numpy(), tokenizer)\n",
    "validation_dataset = CustomDataset(validation_df['text'].to_numpy(), validation_df['labels'].to_numpy(), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We configure and train the RoBERTa model for classification.\n",
    "The training parameters `num_train_epochs`, `batch_size`, `weight_decay`, `learning_rate`, and `adam_epsilon` were carefully selected based on the recommendations provided in a recent study.\n",
    "\n",
    "Additional `TrainingArguments` parameters such as `load_best_model_at_end`, `metric_for_best_model`, and `greater_is_better` provide advanced, automated control over the training process, allowing for optimal model selection and saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    report = classification_report(labels, preds, output_dict=True)\n",
    "    return report\n",
    "    \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=results_path,          # Directory where to save the trained models\n",
    "    num_train_epochs=4,              # Total number of training epochs\n",
    "    per_device_train_batch_size=32,  # Batch size for training\n",
    "    per_device_eval_batch_size=32,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps\n",
    "    weight_decay=0.01,               # Weight decay if applicable\n",
    "    logging_dir='./logs',            # Directory where to save logs\n",
    "    evaluation_strategy=\"epoch\",     # Evaluation strategy to adopt during training\n",
    "    save_strategy=\"epoch\",           # Save the model at the end of each epoch\n",
    "    learning_rate=2e-5,              # Learning rate specified in the paper\n",
    "    adam_epsilon=1e-8,               # Can be \"no\", \"steps\", or \"epoch\"\n",
    "    eval_steps=100,                  # Number of training steps between two evaluations\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Year by Year\n",
    "To test the model on data from subsequent years, one at a time, we load the data for each year after 2007 and evaluate the model on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_year = {}\n",
    "\n",
    "def evaluate_model_for_year(model, tokenizer, year, df):\n",
    "    test_df = df[df['date'].dt.year == year]\n",
    "    if test_df.empty:\n",
    "        print(f\"No data for year {year}\")\n",
    "        return None\n",
    "    \n",
    "    test_df = test_df.copy()\n",
    "    test_df['text'] = test_df['title'] + \" \" + test_df['body']\n",
    "    test_df['labels'] = test_df['label'].apply(lambda x: 1 if x == 'Enhancement' else 0)\n",
    "    \n",
    "    test_dataset = CustomDataset(test_df['text'].to_numpy(), test_df['labels'].to_numpy(), tokenizer)\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    metrics = compute_metrics(predictions)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate_model_for_range(model, tokenizer, start_year, end_year, df):\n",
    "    test_df = df[(df['date'].dt.year >= start_year) & (df['date'].dt.year <= end_year)]\n",
    "    if test_df.empty:\n",
    "        print(f\"No data for range {start_year}-{end_year}\")\n",
    "        return None\n",
    "    \n",
    "    test_df = test_df.copy()\n",
    "    test_df['text'] = test_df['title'] + \" \" + test_df['body']\n",
    "    test_df['labels'] = test_df['label'].apply(lambda x: 1 if x == 'Enhancement' else 0)\n",
    "    \n",
    "    test_dataset = CustomDataset(test_df['text'].to_numpy(), test_df['labels'].to_numpy(), tokenizer)\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    metrics = compute_metrics(predictions)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluation for the range start_year_train - end_year_train\n",
    "print(f\"Testing the year range {start_year_train} - {end_year_train} ...\")\n",
    "range_metrics = evaluate_model_for_range(model, tokenizer, start_year_train, end_year_train, df_train_val)\n",
    "if range_metrics:\n",
    "    results_by_year[f\"{start_year_train}-{end_year_train}\"] = range_metrics\n",
    "\n",
    "# Evaluation for each subsequent year\n",
    "for year in range(end_year_train + 1, last_year_test):\n",
    "    print(f\"Testing the year {year}...\")\n",
    "    year_metrics = evaluate_model_for_year(model, tokenizer, year, df_all)\n",
    "    if year_metrics:\n",
    "        results_by_year[year] = year_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of methods for generating plots\n",
    "We define a parametric method for generating plots with respect to the desired metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(results, metrics, title, start_ylim=None, end_ylim=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    years = list(results.keys())\n",
    "    \n",
    "    if start_ylim is not None and end_ylim is not None and start_ylim < end_ylim:\n",
    "        plt.ylim(start_ylim, end_ylim)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric == 'accuracy':\n",
    "            values = [results[year][metric] for year in years]\n",
    "        else:\n",
    "            values = [results[year]['macro avg'][metric] for year in years]\n",
    "        plt.plot(years, values, label=metric.capitalize(), marker='o')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot to file\n",
    "    file_name = f\"{results_path}/{title.replace(' ', '_').lower()}.png\"\n",
    "    plt.savefig(file_name)\n",
    "    print(f\"Plot saved: {file_name}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_class_metrics(results, classes, metrics, title_prefix, start_ylim=None, end_ylim=None):\n",
    "    years = list(results.keys())\n",
    "\n",
    "    for index, class_name in enumerate(classes):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric_values = [results[year][str(index)][metric] for year in years if str(index) in results[year]]\n",
    "            plt.plot(years, metric_values, label=f'{metric} ({class_name})', marker='o')\n",
    "\n",
    "        plt.title(f'{title_prefix} for {class_name}')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.xticks(rotation=45)\n",
    "        if start_ylim is not None and end_ylim is not None:\n",
    "            plt.ylim(start_ylim, end_ylim)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot to file\n",
    "        file_name = f\"{results_path}/{title_prefix.replace(' ', '_').lower()}_{class_name.lower()}.png\"\n",
    "        plt.savefig(file_name)\n",
    "        print(f\"Plot saved: {file_name}\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Printing [1]\n",
    "Precision, Recall, F1-Score, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Plot: Precision and Recall by Year (ylim: 0-1)\n",
    "plot_metrics(results_by_year, ['precision', 'recall'], 'Precision and Recall by Year', 0, 1)\n",
    "# Print Plot: F1 Score by Year (ylim: None-None)\n",
    "plot_metrics(results_by_year, ['precision', 'recall'], 'Precision and Recall by Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Plot: F1 Score and Accuracy by Year (ylim: 0-1)\n",
    "plot_metrics(results_by_year, ['f1-score', 'accuracy'], 'F1 Score and Accuracy by Year', 0, 1)\n",
    "# Print Plot: F1 Score and Accuracy by Year (ylim: None-None)\n",
    "plot_metrics(results_by_year, ['f1-score', 'accuracy'], 'F1 Score and Accuracy by Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Printing [2]\n",
    "Metrics by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Bug', 'Enhancement'] \n",
    "metrics = ['precision', 'recall']\n",
    "# Print Plot: Class Metrics for each class (ylim: 0-1)\n",
    "plot_class_metrics(results_by_year, classes, metrics, 'Class Metrics', 0, 1)\n",
    "# Print Plot: Class Metrics for each class (ylim: None-None)\n",
    "plot_class_metrics(results_by_year, classes, metrics, 'Class Metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure of Corpus Variability\n",
    "It measures how similar the embeddings (vector representations of text) of two data sets (for example, the training set and the test set for a given year) are to each other. A higher value indicates greater similarity, which may suggest that the two datasets have a similar linguistic distribution or cover related topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize a list to store the annual average similarity\n",
    "yearly_similarity = []\n",
    "\n",
    "# Calculation of cosine similarity for each year of testing\n",
    "for year in range(end_year_train + 1, last_year_test + 1):\n",
    "    test_df = df_all[df_all['date'].dt.year == year]\n",
    "\n",
    "    if test_df.empty:\n",
    "        print(f\"No data for year {year}\")\n",
    "        yearly_similarity.append(None)  # Adds None for years with no data\n",
    "        continue\n",
    "    \n",
    "    # Handle NaN with empty strings\n",
    "    test_df['title'].fillna('', inplace=True)\n",
    "    test_df['body'].fillna('', inplace=True)\n",
    "    train_df['title'].fillna('', inplace=True)\n",
    "    train_df['body'].fillna('', inplace=True)\n",
    "\n",
    "    # Prepare the texts\n",
    "    test_texts = test_df['title'] + \" \" + test_df['body']\n",
    "    train_texts = train_df['title'] + \" \" + train_df['body']\n",
    "    \n",
    "    # Compute embeddings\n",
    "    train_embeddings = model.encode(train_texts.tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "    test_embeddings = model.encode(test_texts.tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = util.pytorch_cos_sim(train_embeddings, test_embeddings)\n",
    "    yearly_similarity.append(similarity_matrix.mean().item())\n",
    "\n",
    "# Plot the average cosine similarity by year\n",
    "plt.figure(figsize=(10, 6))\n",
    "years = range(end_year_train + 1, last_year_test + 1)\n",
    "plt.plot(years, yearly_similarity, marker='o')\n",
    "plt.title('Average Cosine Similarity per Year')\n",
    "plt.xticks(years)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Cosine Similarity')\n",
    "plt.grid(True)\n",
    "# Save the plot to a file\n",
    "plot_file_name = f\"{results_path}/cosine_similarity_year.png\"\n",
    "plt.savefig(plot_file_name)\n",
    "print(f\"Plot saved: {plot_file_name}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drift Detection\n",
    "We'll use the alibi-detect library to implement drift detection on text data processed by RoBERTa model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all-MiniLM-L6-v2 model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def calculate_embeddings(texts):\n",
    "    return model.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# Embeddings for the training data (Handle NaN with empty strings)\n",
    "train_texts = train_df['title'].fillna('') + \" \" + train_df['body'].fillna('')\n",
    "train_embeddings = calculate_embeddings(train_texts.tolist())\n",
    "\n",
    "# Autoencoder\n",
    "enc_dim = 32\n",
    "shape = train_embeddings.shape[1:]\n",
    "uae = UAE(shape=shape, enc_dim=enc_dim)\n",
    "\n",
    "# Initialize KSDrift detector\n",
    "ks_drift = KSDrift(train_embeddings, p_val=0.05)\n",
    "\n",
    "# Dictionary to store drift detection results\n",
    "drift_results = {}\n",
    "\n",
    "# Check for drift in each year\n",
    "for year in range(end_year_train + 1, last_year_test + 1):\n",
    "    test_df = df_all[df_all['date'].dt.year == year].copy()\n",
    "    if test_df.empty:\n",
    "        print(f\"No data for year {year}\")\n",
    "        continue\n",
    "\n",
    "    # Prepare test data\n",
    "    test_texts = test_df['title'].fillna('') + \" \" + test_df['body'].fillna('')\n",
    "    test_embeddings = calculate_embeddings(test_texts.tolist())\n",
    "\n",
    "    # Perform drift detection\n",
    "    preds = ks_drift.predict(test_embeddings)\n",
    "    drift_results[year] = {'data_drift': preds['data']['is_drift'], 'p_value': preds['data']['p_val']}\n",
    "\n",
    "# Convert the drift results dictionary to a DataFrame for plotting\n",
    "df_drift_results = pd.DataFrame.from_dict(drift_results, orient='index', columns=['data_drift', 'p_value'])\n",
    "df_drift_results.index.name = 'Year'\n",
    "df_drift_results.reset_index(inplace=True)\n",
    "df_drift_results['p_value_mean'] = df_drift_results['p_value'].apply(np.mean)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_drift_results['Year'], df_drift_results['p_value_mean'], marker='o')\n",
    "plt.title('Data Drift Detection Results')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Data Drift Detected')\n",
    "plt.xticks(df_drift_results['Year'], rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plot_file_name = f\"{results_path}/drift_detection_results.png\"\n",
    "plt.savefig(plot_file_name)\n",
    "print(f\"Plot saved: {plot_file_name}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archiving of Results\n",
    "\n",
    "We compress and save the training results exluding checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_results(results_dir=results_path, zip_name='results.zip'):\n",
    "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(results_dir):\n",
    "            dirs[:] = [d for d in dirs if 'checkpoint' not in d]\n",
    "            for file in files:\n",
    "                if 'checkpoint' not in root:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    zipf.write(file_path, os.path.relpath(file_path, start=os.path.join(results_dir, '..')))\n",
    "    print(f\"Results archived in {zip_name}\")\n",
    "\n",
    "# Call the function to create the zip archive\n",
    "zip_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
