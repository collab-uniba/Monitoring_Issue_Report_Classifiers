{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Bug and Enhancement Reports with RoBERTa\n",
    "\n",
    "This notebook demonstrates how to train a RoBERTa model for bug and enhancement report classification using the Hugging Face `transformers` framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install torch torchvision\n",
    "%pip install transformers\n",
    "%pip install scikit-learn\n",
    "%pip install accelerate\n",
    "%pip install imbalanced-learn\n",
    "%pip install sentence-transformers\n",
    "%pip install alibi-detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Let's start by importing all the libraries needed for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from alibi_detect.cd import KSDrift\n",
    "from alibi_detect.cd.tensorflow import UAE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of Dataset and Parameters\n",
    "Write the parameters in the config.yaml file:\n",
    "- `dataset_path`: defines the path of the dataset in CSV format\n",
    "- `start_year_train`: defines the start year of the train dataset\n",
    "- `end_year_train`: defines the end year of the train dataset\n",
    "- `undersampling_flag`: defines with a boolean whether to perform undersampling\n",
    "- `start_year`: defines the start year for the test dataset\n",
    "- `start_month`: defines the start month for the test dataset\n",
    "- `end_year`: defines the end year for the test dataset\n",
    "- `end_month`: defines the end month for the test dataset\n",
    "\n",
    "*NB*: \n",
    "- `start_year_train` and `end_year_train` are the range of years that make up the train dataset\n",
    "- `start-year/start-month` and `end-year/end-month` make up two dates which will be the range in which the code will start testing, similarity and drift detection.\n",
    "Each phase will analyze month by month within that range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "config_path = './config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "dataset_path = config['dataset_path']\n",
    "start_year_train = config['start_year_train']\n",
    "end_year_train = config['end_year_train']\n",
    "undersampling_flag = config['undersampling_flag']\n",
    "start_year = config['start_year']\n",
    "end_year = config['end_year']\n",
    "start_month = config['start_month']\n",
    "end_month = config['end_month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the dataset name from the dataset_path\n",
    "dataset_name = os.path.basename(dataset_path).split('.')[0]\n",
    "\n",
    "# Defining the results and model save paths using the dataset name\n",
    "results_path = f'./RESULTS/{dataset_name}'\n",
    "\n",
    "model_save_path = os.path.join(results_path, \"model\")\n",
    "\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "    \n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We load the data from the CSV file and prepare it for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_all = pd.read_csv(dataset_path)\n",
    "df_all['date'] = pd.to_datetime(df_all['date'], errors='coerce', format='%Y-%m-%dT%H:%M:%S.%f+0000')\n",
    "df_all = df_all[df_all['label'].isin(['Bug', 'Enhancement'])]\n",
    "\n",
    "# Filter data for training and validation (start_year_train - end_year_train)\n",
    "df_train_val = df_all[(df_all['date'].dt.year >= start_year_train) & (df_all['date'].dt.year <= end_year_train)]\n",
    "df_train_val['text'] = df_train_val['title'] + \" \" + df_train_val['body']\n",
    "df_train_val['labels'] = df_train_val['label'].apply(lambda x: 1 if x == 'Enhancement' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Division in Train e Validation\n",
    "We split the data in train (70%) and validation (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df = train_test_split(df_train_val, test_size=0.3, random_state=42, stratify=df_train_val['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling\n",
    "\n",
    "This code initializes a RandomUnderSampler, fits it to the training data, and creates a new dataframe (train_df_resampled) with the undersampled data.\n",
    "You can then use train_df_resampled to train the model instead of train_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersampling_flag:\n",
    "\n",
    "    # Initialize the RandomUnderSampler\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "    # Resample the training data\n",
    "    x_train_resampled, y_train_resampled = rus.fit_resample(train_df.drop('labels', axis=1), train_df['labels'])\n",
    "\n",
    "    # Reconstruct the training dataframe with the resampled data\n",
    "    train_df = pd.concat([x_train_resampled, y_train_resampled], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the Dataset\n",
    "\n",
    "We define a `CustomDataset` class to prepare the data for training with BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_token_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        if pd.isna(text):\n",
    "            text = \"\"  # Replaces NaN values â€‹â€‹with empty strings\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_save_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
    "train_dataset = CustomDataset(train_df['text'].to_numpy(), train_df['labels'].to_numpy(), tokenizer)\n",
    "validation_dataset = CustomDataset(validation_df['text'].to_numpy(), validation_df['labels'].to_numpy(), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    report = classification_report(labels, preds, output_dict=True)\n",
    "    return report\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=results_path,         # Directory where to save the trained models\n",
    "    per_device_train_batch_size=32,  # Batch size for training\n",
    "    per_device_eval_batch_size=32,   # Batch size for evaluation\n",
    "    do_train=False,                  # Prevents training from starting\n",
    "    do_eval=True,                    # Enable evaluation\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=validation_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The start date is the first day of the start_year-start_month\n",
    "start_date = pd.Timestamp(year=start_year, month=start_month, day=1)\n",
    "# The end date is the last day of the end_year-end_month\n",
    "end_date = pd.Timestamp(year=end_year, month=end_month, day=1) + relativedelta(months=1) - relativedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Month by Month\n",
    "To test the model on data from subsequent months, one at a time, we load the data for each month and evaluate the model on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_for_month(model, tokenizer, year, month, df):\n",
    "    start_date = pd.Timestamp(year=year, month=month, day=1)\n",
    "    end_date = start_date + relativedelta(months=1)\n",
    "\n",
    "    test_df = df[(df['date'] >= start_date) & (df['date'] < end_date)]\n",
    "    if test_df.empty:\n",
    "        print(f\"No data for {year}-{month}\")\n",
    "        return None\n",
    "    \n",
    "    test_df['text'] = test_df['title'] + \" \" + test_df['body']\n",
    "    test_df['labels'] = test_df['label'].apply(lambda x: 1 if x == 'Enhancement' else 0)\n",
    "    \n",
    "    test_dataset = CustomDataset(test_df['text'].to_numpy(), test_df['labels'].to_numpy(), tokenizer)\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    metrics = compute_metrics(predictions)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "results_by_month = {}\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    year = current_date.year\n",
    "    month = current_date.month\n",
    "    print(f\"Testing for {year}-{month:02d}...\")\n",
    "    month_metrics = evaluate_model_for_month(model, tokenizer, year, month, df_all)\n",
    "    if month_metrics:\n",
    "        results_by_month[f\"{year}-{month:02d}\"] = month_metrics\n",
    "    current_date += relativedelta(months=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test results to a DataFrame\n",
    "test_results_df = pd.DataFrame.from_dict(results_by_month, orient='index').reset_index()\n",
    "test_results_df.columns = ['Month', 'Metrics']\n",
    "test_results_file_path = f\"{results_path}/test_results_monthly.csv\"\n",
    "test_results_df.to_csv(test_results_file_path, index=False)\n",
    "print(f\"Test results saved to {test_results_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of methods for generating plots\n",
    "We define a parametric method for generating plots with respect to the desired metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(results, metrics, title, start_ylim=None, end_ylim=None):\n",
    "    # Every Six Months\n",
    "    def plot_half_yearly(periods, results, title):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        if start_ylim is not None and end_ylim is not None and start_ylim < end_ylim:\n",
    "            plt.ylim(start_ylim, end_ylim)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric == 'accuracy':\n",
    "                values = [results[period][metric] for period in periods]\n",
    "            else:\n",
    "                values = [results[period]['macro avg'][metric] for period in periods]\n",
    "            plt.plot(periods, values, label=metric.capitalize(), marker='o')\n",
    "        \n",
    "        plt.title(title + \" (Every 6 months)\")\n",
    "        plt.xlabel('Year-Month')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.xticks(periods[::6], rotation=90)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        file_name = f\"{results_path}/{title.replace(' ', '_').lower()}_every_6_months.png\"\n",
    "        plt.savefig(file_name)\n",
    "        print(f\"Plot saved: {file_name}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    # Every Year\n",
    "    def plot_yearly(periods, results, title):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        if start_ylim is not None and end_ylim is not None and start_ylim < end_ylim:\n",
    "            plt.ylim(start_ylim, end_ylim)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric == 'accuracy':\n",
    "                values = [results[period][metric] for period in periods]\n",
    "            else:\n",
    "                values = [results[period]['macro avg'][metric] for period in periods]\n",
    "            plt.plot(periods, values, label=metric.capitalize(), marker='o')\n",
    "        \n",
    "        plt.title(title + \" (Every 12 months)\")\n",
    "        plt.xlabel('Year-Month')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        annual_ticks = [period for period in periods if period.endswith('-01')]\n",
    "        plt.xticks(annual_ticks, rotation=90)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        file_name = f\"{results_path}/{title.replace(' ', '_').lower()}_every_6_months.png\"\n",
    "        plt.savefig(file_name)\n",
    "        print(f\"Plot saved: {file_name}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    periods = sorted(results.keys())\n",
    "        \n",
    "    plot_half_yearly(periods, results, title)\n",
    "    plot_yearly(periods, results, title)\n",
    "    \n",
    "def plot_class_metrics(results, classes, metrics, title_prefix, start_ylim=None, end_ylim=None):\n",
    "    periods = sorted(results.keys())\n",
    "    \n",
    "    # Every Six Months\n",
    "    def plot_half_yearly(periods, results):\n",
    "        for index, class_name in enumerate(classes):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "\n",
    "            for metric in metrics:\n",
    "                metric_values = [results[period][str(index)][metric] for period in periods if str(index) in results[period]]\n",
    "                plt.plot(periods, metric_values, label=f'{metric} ({class_name})', marker='o')\n",
    "\n",
    "            plt.title(f'{title_prefix} for {class_name} Every 6 Months')\n",
    "            plt.xlabel('Year-Month')\n",
    "            plt.ylabel('Score')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.xticks(periods[::6], rotation=90)\n",
    "            if start_ylim is not None and end_ylim is not None:\n",
    "                plt.ylim(start_ylim, end_ylim)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save the plot to file\n",
    "            file_name = f\"{results_path}/{title_prefix.replace(' ', '_').lower()}_{class_name.lower()}.png\"\n",
    "            plt.savefig(file_name)\n",
    "            print(f\"Plot saved: {file_name}\")\n",
    "            \n",
    "            plt.show()\n",
    "    \n",
    "    # Every Year\n",
    "    def plot_yearly(periods, results):\n",
    "        for index, class_name in enumerate(classes):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "\n",
    "            for metric in metrics:\n",
    "                metric_values = [results[period][str(index)][metric] for period in periods if str(index) in results[period]]\n",
    "                plt.plot(periods, metric_values, label=f'{metric} ({class_name})', marker='o')\n",
    "\n",
    "            plt.title(f'{title_prefix} for {class_name} Every Year')\n",
    "            plt.xlabel('Year-Month')\n",
    "            plt.ylabel('Score')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            annual_ticks = [period for period in periods if period.endswith('-01')]\n",
    "            plt.xticks(annual_ticks, rotation=90)\n",
    "            if start_ylim is not None and end_ylim is not None:\n",
    "                plt.ylim(start_ylim, end_ylim)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save the plot to file\n",
    "            file_name = f\"{results_path}/{title_prefix.replace(' ', '_').lower()}_{class_name.lower()}.png\"\n",
    "            plt.savefig(file_name)\n",
    "            print(f\"Plot saved: {file_name}\")\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "    plot_half_yearly(periods, results, title_prefix)\n",
    "    plot_yearly(periods, results, title_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Printing [1]\n",
    "Precision, Recall, F1-Score, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Plot: Precision and Recall by Month (ylim: 0-1)\n",
    "plot_metrics(results_by_month, ['precision', 'recall'], 'Precision and Recall by Month', 0, 1)\n",
    "# Print Plot: F1 Score by Month (ylim: None-None)\n",
    "plot_metrics(results_by_month, ['precision', 'recall'], 'Precision and Recall by Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Plot: F1 Score and Accuracy by Month (ylim: 0-1)\n",
    "plot_metrics(results_by_month, ['f1-score', 'accuracy'], 'F1 Score and Accuracy by Month', 0, 1)\n",
    "# Print Plot: F1 Score and Accuracy by Month (ylim: None-None)\n",
    "plot_metrics(results_by_month, ['f1-score', 'accuracy'], 'F1 Score and Accuracy by Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Printing [2]\n",
    "Metrics by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Bug', 'Enhancement'] \n",
    "metrics = ['precision', 'recall']\n",
    "# Print Plot: Class Metrics for each class (ylim: 0-1)\n",
    "plot_class_metrics(results_by_month, classes, metrics, 'Class Metrics', 0, 1)\n",
    "# Print Plot: Class Metrics for each class (ylim: None-None)\n",
    "plot_class_metrics(results_by_month, classes, metrics, 'Class Metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sentence-Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l6 = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure of Corpus Variability\n",
    "It measures how similar the embeddings (vector representations of text) of two data sets (for example, the training set and the test set for a given month) are to each other. A higher value indicates greater similarity, which may suggest that the two datasets have a similar linguistic distribution or cover related topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l6 = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize a list to store the average cosine similarity by month\n",
    "monthly_similarity = []\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    year = current_date.year\n",
    "    month = current_date.month\n",
    "    test_df = df_all[(df_all['date'].dt.year == year) & (df_all['date'].dt.month == month)]\n",
    "    \n",
    "    current_date += relativedelta(months=1)\n",
    "\n",
    "    if test_df.empty:\n",
    "        print(f\"No data for {year}-{month}\")\n",
    "        monthly_similarity.append(None)\n",
    "        continue\n",
    "    \n",
    "    # Handle NaN with empty strings\n",
    "    test_df['title'].fillna('', inplace=True)\n",
    "    test_df['body'].fillna('', inplace=True)\n",
    "    train_df['title'].fillna('', inplace=True)\n",
    "    train_df['body'].fillna('', inplace=True)\n",
    "    \n",
    "    # Prepare the texts\n",
    "    test_texts = test_df['title'] + \" \" + test_df['body']\n",
    "    train_texts = train_df['title'] + \" \" + train_df['body']\n",
    "    \n",
    "    # Compute embeddings\n",
    "    train_embeddings = model_l6.encode(train_texts.tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "    test_embeddings = model_l6.encode(test_texts.tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = util.pytorch_cos_sim(train_embeddings, test_embeddings)\n",
    "    monthly_similarity.append(similarity_matrix.mean().item())\n",
    "\n",
    "# Create a list of monthly periods from start to finish\n",
    "monthly_periods = pd.date_range(start=start_date, end=end_date, freq='M').strftime('%Y-%m').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for cosine similarity\n",
    "cosine_similarity_df = pd.DataFrame({\n",
    "    'Month': monthly_periods,\n",
    "    'Cosine Similarity': monthly_similarity\n",
    "})\n",
    "cosine_similarity_file_path = f\"{results_path}/cosine_similarity_monthly.csv\"\n",
    "cosine_similarity_df.to_csv(cosine_similarity_file_path, index=False)\n",
    "print(f\"Cosine similarity data saved to {cosine_similarity_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Cosine Similarity Labeled Every 6 Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average cosine similarity by month\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(monthly_periods, monthly_similarity, marker='o')\n",
    "plt.title('Average Cosine Similarity per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Cosine Similarity')\n",
    "plt.grid(True)\n",
    "plt.xticks(monthly_periods[::6], rotation=90)\n",
    "# Save the plot to a file\n",
    "plot_file_name = f\"{results_path}/cosine_similarity_month.png\"\n",
    "plt.savefig(plot_file_name)\n",
    "print(f\"Plot saved: {plot_file_name}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Cosine Similarity Labeled Every Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average cosine similarity by month\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(monthly_periods, monthly_similarity, marker='o')\n",
    "plt.title('Average Cosine Similarity per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Cosine Similarity')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=90)\n",
    "annual_ticks = [period for period in monthly_periods if period.endswith('-01')]\n",
    "plt.xticks(annual_ticks, rotation=90)\n",
    "# Save the plot to a file\n",
    "plot_file_name = f\"{results_path}/cosine_similarity_month.png\"\n",
    "plt.savefig(plot_file_name)\n",
    "print(f\"Plot saved: {plot_file_name}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drift Detection\n",
    "We'll use the alibi-detect library to implement drift detection on text data processed by RoBERTa model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings(texts):\n",
    "    return model_l6.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# Embeddings for the training data (Handle NaN with empty strings)\n",
    "train_texts = train_df['title'].fillna('') + \" \" + train_df['body'].fillna('')\n",
    "train_embeddings = calculate_embeddings(train_texts.tolist())\n",
    "\n",
    "# Autoencoder\n",
    "enc_dim = 32\n",
    "shape = train_embeddings.shape[1:]\n",
    "uae = UAE(shape=shape, enc_dim=enc_dim)\n",
    "\n",
    "# Initialize KSDrift detector\n",
    "ks_drift = KSDrift(train_embeddings, p_val=0.05)\n",
    "\n",
    "# Dictionary to store drift detection results\n",
    "drift_results = {}\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    year = current_date.year\n",
    "    month = current_date.month\n",
    "    test_df = df_all[(df_all['date'].dt.year == year) & (df_all['date'].dt.month == month)]\n",
    "    \n",
    "    current_date += relativedelta(months=1)\n",
    "\n",
    "    if test_df.empty:\n",
    "        print(f\"No data for {year}-{month}\")\n",
    "        current_date += relativedelta(months=1)\n",
    "        continue\n",
    "\n",
    "    test_texts = test_df['title'].fillna('') + \" \" + test_df['body'].fillna('')\n",
    "    test_embeddings = calculate_embeddings(test_texts.tolist())\n",
    "\n",
    "    preds = ks_drift.predict(test_embeddings)\n",
    "    drift_results[f\"{year}-{month:02d}\"] = {\n",
    "        'data_drift': preds['data']['is_drift'],\n",
    "        'p_value': preds['data']['p_val'],\n",
    "        'd_statistic': preds['data']['distance']\n",
    "    }\n",
    "\n",
    "# Convert the drift results dictionary to a DataFrame\n",
    "drift_results_df = pd.DataFrame.from_dict(drift_results, orient='index', columns=['data_drift', 'p_value', 'd_statistic'])\n",
    "drift_results_df.index.name = 'Period'\n",
    "drift_results_df.reset_index(inplace=True)\n",
    "drift_results_df[['Year', 'Month']] = drift_results_df['Period'].str.split('-', expand=True)\n",
    "\n",
    "# Sort the DataFrame by year and month\n",
    "drift_results_df.sort_values(by=['Year', 'Month'], inplace=True)\n",
    "drift_results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the drift detection results\n",
    "print(drift_results_df[['Year', 'Month', 'data_drift', 'p_value', 'd_statistic']])\n",
    "\n",
    "# Save the drift detection results to a CSV file\n",
    "drift_results_csv_path = f\"{results_path}/drift_detection_results_monthly.csv\"\n",
    "drift_results_df[['Year', 'Month', 'data_drift', 'p_value', 'd_statistic']].to_csv(drift_results_csv_path, index=False)\n",
    "print(f\"Drift detection results saved to {drift_results_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archiving of Results\n",
    "\n",
    "We compress and save the training results exluding checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_results(results_dir=results_path, zip_name='results.zip'):\n",
    "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(results_dir):\n",
    "            dirs[:] = [d for d in dirs if 'checkpoint' not in d]\n",
    "            for file in files:\n",
    "                if 'checkpoint' not in root:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    zipf.write(file_path, os.path.relpath(file_path, start=os.path.join(results_dir, '..')))\n",
    "    print(f\"Results archived in {zip_name}\")\n",
    "\n",
    "# Call the function to create the zip archive\n",
    "zip_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
