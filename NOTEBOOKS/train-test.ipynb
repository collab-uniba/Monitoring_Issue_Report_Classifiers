{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Bug and Enhancement Reports with BERT\n",
    "\n",
    "This notebook demonstrates how to train a BERT model for bug and enhancement report classification using the Hugging Face `transformers` framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install transformers\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Let's start by importing all the libraries needed for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = './RESULTS' \n",
    "\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We load the data from the CSV file and prepare it for training and evaluation. \n",
    "We filter the data to include only those between 2000 and 2007 and correctly label them as Bug (0) or Enhancement (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the complete dataset\n",
    "df_path = \"../CSV/STANDARD/MAPPED/Apache.csv\"\n",
    "df_all = pd.read_csv(df_path)\n",
    "df_all['date'] = pd.to_datetime(df_all['date'], errors='coerce', format='%Y-%m-%dT%H:%M:%S.%f+0000')\n",
    "df_all = df_all[df_all['label'].isin(['Bug', 'Enhancement'])]\n",
    "\n",
    "# Filter data for training and validation (2000-2007)\n",
    "df_train_val = df_all[(df_all['date'].dt.year >= 2000) & (df_all['date'].dt.year <= 2007)]\n",
    "df_train_val['text'] = df_train_val['title'] + \" \" + df_train_val['body']\n",
    "df_train_val['labels'] = df_train_val['label'].apply(lambda x: 1 if x == 'Enhancement' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Division in Train e Validation\n",
    "We split the data in train (70%) and validation (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df = train_test_split(df_train_val, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the Dataset\n",
    "\n",
    "We define a `CustomDataset` class to prepare the data for training with BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_token_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We use the BERT tokenizer to convert text into tokens that the model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "train_dataset = CustomDataset(train_df['text'].to_numpy(), train_df['labels'].to_numpy(), tokenizer)\n",
    "validation_dataset = CustomDataset(validation_df['text'].to_numpy(), validation_df['labels'].to_numpy(), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We configure and train the BERT model for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "    \n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=results_dir,          # Directory where to save the trained models\n",
    "    num_train_epochs=3,              # Total number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=64,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps\n",
    "    weight_decay=0.01,               # Weight decay if applicable\n",
    "    logging_dir='./logs',            # Directory where to save logs\n",
    "    evaluation_strategy=\"epoch\",     # Can be \"no\", \"steps\", or \"epoch\"\n",
    "    eval_steps=100,                  # Number of training steps between two evaluations\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Year by Year\n",
    "To test the model on data from subsequent years, one at a time, we load the data for each year after 2007 and evaluate the model on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_year = {}\n",
    "\n",
    "def evaluate_model_for_year(model, tokenizer, year, df_all):\n",
    "    test_df = df_all[df_all['date'].dt.year == year]\n",
    "    if test_df.empty:\n",
    "        print(f\"No data for year {year}\")\n",
    "        return None\n",
    "    \n",
    "    test_df['text'] = test_df['title'] + \" \" + test_df['body']\n",
    "    test_dataset = CustomDataset(test_df['text'].to_numpy(), test_df['labels'].to_numpy(), tokenizer)\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    metrics = compute_metrics(predictions)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "for year in range(2008, 2023):\n",
    "    metrics = evaluate_model_for_year(model, tokenizer, year, df_all)\n",
    "    if metrics:\n",
    "        results_by_year[year] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results_by_year):\n",
    "    years = list(results_by_year.keys())\n",
    "    accuracies = [results_by_year[year]['accuracy'] for year in years]\n",
    "    f1_scores = [results_by_year[year]['f1'] for year in years]\n",
    "    precisions = [results_by_year[year]['precision'] for year in years]\n",
    "    recalls = [results_by_year[year]['recall'] for year in years]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(years, accuracies, label='Accuracy', marker='o')\n",
    "    plt.plot(years, f1_scores, label='F1 Score', marker='o')\n",
    "    plt.plot(years, precisions, label='Precision', marker='o')\n",
    "    plt.plot(years, recalls, label='Recall', marker='o')\n",
    "    plt.title('Model Performance by Year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(years, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Mostra il grafico\n",
    "    plt.show()\n",
    "    \n",
    "    # Salva il grafico su file\n",
    "    plt.savefig('./RESULTS/model_performance_by_year.png')\n",
    "\n",
    "# Visualizza e salva i risultati\n",
    "plot_results(results_by_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archiving of Results\n",
    "\n",
    "We compress and save the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_results(results_dir=results_dir, zip_name='results.zip'):\n",
    "    with zipfile.ZipFile(zip_name, 'w') as zipf:\n",
    "        for root, dirs, files in os.walk(results_dir):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(results_dir, '..')))\n",
    "\n",
    "zip_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
