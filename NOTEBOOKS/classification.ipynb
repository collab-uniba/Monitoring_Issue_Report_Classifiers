{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Bug and Enhancement Reports with RoBERTa\n",
    "\n",
    "This notebook demonstrates how to train a RoBERTa model for bug and enhancement report classification using the Hugging Face `transformers` framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install torch torchvision\n",
    "%pip install transformers\n",
    "%pip install scikit-learn\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Let's start by importing all the libraries needed for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of Dataset and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../CSV/STANDARD/MAPPED/Apache.csv\"  # Path to the dataset\n",
    "start_year_train = 2000  # Start year for the training\n",
    "end_year_train = 2004  # End year for the training\n",
    "last_year_test = 2023 # Last year for the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the dataset name from the dataset_path\n",
    "dataset_name = os.path.basename(dataset_path).split('.')[0]\n",
    "\n",
    "# Defining the results and model save paths using the dataset name\n",
    "results_path = f'./RESULTS/{dataset_name}'\n",
    "\n",
    "model_save_path = os.path.join(results_path, \"model\")\n",
    "\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)\n",
    "    \n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We load the data from the CSV file and prepare it for training and evaluation. \n",
    "We filter the data to include only those between 2000 and 2007 and correctly label them as Bug (0) or Enhancement (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_all = pd.read_csv(dataset_path)\n",
    "df_all['date'] = pd.to_datetime(df_all['date'], errors='coerce', format='%Y-%m-%dT%H:%M:%S.%f+0000')\n",
    "df_all = df_all[df_all['label'].isin(['Bug', 'Enhancement'])]\n",
    "\n",
    "# Filter data for training and validation (start_year_train - end_year_train)\n",
    "df_train_val = df_all[(df_all['date'].dt.year >= start_year_train) & (df_all['date'].dt.year <= end_year_train)]\n",
    "df_train_val['text'] = df_train_val['title'] + \" \" + df_train_val['body']\n",
    "df_train_val['labels'] = df_train_val['label'].apply(lambda x: 1 if x == 'Enhancement' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Division in Train e Validation\n",
    "We split the data in train (70%) and validation (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df = train_test_split(df_train_val, test_size=0.3, random_state=42, stratify=df_train_val['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the Dataset\n",
    "\n",
    "We define a `CustomDataset` class to prepare the data for training with BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_token_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        if pd.isna(text):\n",
    "            text = \"\"  # Replaces NaN values â€‹â€‹with empty strings\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We use the BERT tokenizer to convert text into tokens that the model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_dataset = CustomDataset(train_df['text'].to_numpy(), train_df['labels'].to_numpy(), tokenizer)\n",
    "validation_dataset = CustomDataset(validation_df['text'].to_numpy(), validation_df['labels'].to_numpy(), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We configure and train the RoBERTa model for classification.\n",
    "The training parameters `num_train_epochs`, `batch_size`, `weight_decay`, `learning_rate`, and `adam_epsilon` were carefully selected based on the recommendations provided in a recent study.\n",
    "\n",
    "Additional `TrainingArguments` parameters such as `load_best_model_at_end`, `metric_for_best_model`, and `greater_is_better` provide advanced, automated control over the training process, allowing for optimal model selection and saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    report = classification_report(labels, preds, output_dict=True)\n",
    "    return report\n",
    "    \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=results_path,          # Directory where to save the trained models\n",
    "    num_train_epochs=4,              # Total number of training epochs\n",
    "    per_device_train_batch_size=32,  # Batch size for training\n",
    "    per_device_eval_batch_size=32,   # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps\n",
    "    weight_decay=0.01,               # Weight decay if applicable\n",
    "    logging_dir='./logs',            # Directory where to save logs\n",
    "    evaluation_strategy=\"epoch\",     # Evaluation strategy to adopt during training\n",
    "    save_strategy=\"epoch\",           # Save the model at the end of each epoch\n",
    "    learning_rate=2e-5,              # Learning rate specified in the paper\n",
    "    adam_epsilon=1e-8,               # Can be \"no\", \"steps\", or \"epoch\"\n",
    "    eval_steps=100,                  # Number of training steps between two evaluations\n",
    "    load_best_model_at_end=True,     # Load the best model at the end of training\n",
    "    metric_for_best_model='f1',      # Use F1 as a metric to select the best model\n",
    "    greater_is_better=True,          # The higher the F1 value, the better the model\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Year by Year\n",
    "To test the model on data from subsequent years, one at a time, we load the data for each year after 2007 and evaluate the model on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_year = {}\n",
    "\n",
    "def evaluate_model_for_year(model, tokenizer, year, df):\n",
    "    test_df = df[df['date'].dt.year == year]\n",
    "    if test_df.empty:\n",
    "        print(f\"No data for year {year}\")\n",
    "        return None\n",
    "    \n",
    "    test_df = test_df.copy()\n",
    "    test_df['text'] = test_df['title'] + \" \" + test_df['body']\n",
    "    test_df['labels'] = test_df['label'].apply(lambda x: 1 if x == 'Enhancement' else 0)\n",
    "    \n",
    "    test_dataset = CustomDataset(test_df['text'].to_numpy(), test_df['labels'].to_numpy(), tokenizer)\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    metrics = compute_metrics(predictions)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate_model_for_range(model, tokenizer, start_year, end_year, df):\n",
    "    test_df = df[(df['date'].dt.year >= start_year) & (df['date'].dt.year <= end_year)]\n",
    "    if test_df.empty:\n",
    "        print(f\"No data for range {start_year}-{end_year}\")\n",
    "        return None\n",
    "    \n",
    "    test_df = test_df.copy()\n",
    "    test_df['text'] = test_df['title'] + \" \" + test_df['body']\n",
    "    test_df['labels'] = test_df['label'].apply(lambda x: 1 if x == 'Enhancement' else 0)\n",
    "    \n",
    "    test_dataset = CustomDataset(test_df['text'].to_numpy(), test_df['labels'].to_numpy(), tokenizer)\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    metrics = compute_metrics(predictions)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluation for the range start_year_train - end_year_train\n",
    "print(f\"Testing the year range {start_year_train} - {end_year_train} ...\")\n",
    "range_metrics = evaluate_model_for_range(model, tokenizer, start_year_train, end_year_train, df_train_val)\n",
    "if range_metrics:\n",
    "    results_by_year[\"start_year-2007\"] = range_metrics\n",
    "\n",
    "# Evaluation for each subsequent year\n",
    "for year in range(end_year_train + 1, last_year_test):\n",
    "    print(f\"Testing the year {year}...\")\n",
    "    year_metrics = evaluate_model_for_year(model, tokenizer, year, df_all)\n",
    "    if year_metrics:\n",
    "        results_by_year[year] = year_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of methods for generating plots\n",
    "We define a parametric method for generating plots with respect to the desired metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(results, metrics, title, start_ylim=None, end_ylim=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    years = list(results.keys())\n",
    "    \n",
    "    if start_ylim is not None and end_ylim is not None and start_ylim < end_ylim:\n",
    "        plt.ylim(start_ylim, end_ylim)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric == 'accuracy':\n",
    "            values = [results[year][metric] for year in years]\n",
    "        else:\n",
    "            values = [results[year]['macro avg'][metric] for year in years]\n",
    "        plt.plot(years, values, label=metric.capitalize(), marker='o')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the graph to file\n",
    "    file_name = f\"{results_path}/{title.replace(' ', '_').lower()}.png\"\n",
    "    plt.savefig(file_name)\n",
    "    print(f\"Plot saved: {file_name}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_class_metrics(results, classes, metrics, title_prefix, start_ylim=None, end_ylim=None):\n",
    "    years = list(results.keys())\n",
    "\n",
    "    for index, class_name in enumerate(classes):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric_values = [results[year][str(index)][metric] for year in years if str(index) in results[year]]\n",
    "            plt.plot(years, metric_values, label=f'{metric} ({class_name})', marker='o')\n",
    "\n",
    "        plt.title(f'{title_prefix} for {class_name}')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.xticks(rotation=45)\n",
    "        if start_ylim is not None and end_ylim is not None:\n",
    "            plt.ylim(start_ylim, end_ylim)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the graph to file\n",
    "        file_name = f\"{results_path}/{title_prefix.replace(' ', '_').lower()}_{class_name.lower()}.png\"\n",
    "        plt.savefig(file_name)\n",
    "        print(f\"Plot saved: {file_name}\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Printing [1]\n",
    "Precision, Recall, F1-Score, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Plot: Precision and Recall by Year (ylim: 0-1)\n",
    "plot_metrics(results_by_year, ['precision', 'recall'], 'Precision and Recall by Year', 0, 1)\n",
    "# Print Plot: F1 Score by Year (ylim: None-None)\n",
    "plot_metrics(results_by_year, ['precision', 'recall'], 'Precision and Recall by Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Plot: F1 Score and Accuracy by Year (ylim: 0-1)\n",
    "plot_metrics(results_by_year, ['f1-score', 'accuracy'], 'F1 Score and Accuracy by Year', 0, 1)\n",
    "# Print Plot: F1 Score and Accuracy by Year (ylim: None-None)\n",
    "plot_metrics(results_by_year, ['f1-score', 'accuracy'], 'F1 Score and Accuracy by Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Printing [2]\n",
    "Metrics by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Bug', 'Enhancement'] \n",
    "metrics = ['precision', 'recall']\n",
    "# Print Plot: Class Metrics for each class (ylim: 0-1)\n",
    "plot_class_metrics(results_by_year, classes, metrics, 'Class Metrics', 0, 1)\n",
    "# Print Plot: Class Metrics for each class (ylim: None-None)\n",
    "plot_class_metrics(results_by_year, classes, metrics, 'Class Metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archiving of Results\n",
    "\n",
    "We compress and save the training results exluding checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_results(results_dir=results_path, zip_name='results.zip'):\n",
    "    with zipfile.ZipFile(zip_name, 'w') as zipf:\n",
    "        for root, dirs, files in os.walk(results_dir):\n",
    "            # Exclude the checkpoints directory\n",
    "            if 'checkpoints' in dirs:\n",
    "                dirs.remove('checkpoints')\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                zipf.write(file_path, os.path.relpath(file_path, os.path.join(results_dir, '..')))\n",
    "    \n",
    "    print(f\"Results archived in {zip_name}\")\n",
    "\n",
    "# Execute the function to zip results excluding checkpoints\n",
    "zip_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
