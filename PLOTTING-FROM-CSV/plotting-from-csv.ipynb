{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scipy\n",
    "%pip install matplotlib\n",
    "%pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"RedHat\"\n",
    "\n",
    "# Defining the results and model save paths using the dataset name\n",
    "results_path = f'./RESULTS/{dataset_name}'\n",
    "\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "\n",
    "# Load the data\n",
    "test_results_data = pd.read_csv('test_results_monthly.csv', index_col='index')\n",
    "test_results_data.index = pd.to_datetime(test_results_data.index)\n",
    "\n",
    "# Initialize lists to store the extracted data\n",
    "precision_data = []\n",
    "recall_data = []\n",
    "f1_score_data = []\n",
    "accuracy_data = []\n",
    "\n",
    "# Extract the data\n",
    "for row in test_results_data.itertuples():\n",
    "    macro_avg_metrics = literal_eval(row._4)  # Adjust the index if necessary based on your DataFrame\n",
    "    precision_data.append(macro_avg_metrics['precision'])\n",
    "    recall_data.append(macro_avg_metrics['recall'])\n",
    "    f1_score_data.append(macro_avg_metrics['f1-score'])\n",
    "    accuracy_data.append(row.accuracy)  # Assuming 'accuracy' is directly accessible\n",
    "\n",
    "# Create new columns in the DataFrame\n",
    "test_results_data['Precision'] = precision_data\n",
    "test_results_data['Recall'] = recall_data\n",
    "test_results_data['F1-Score'] = f1_score_data\n",
    "test_results_data['Accuracy'] = accuracy_data\n",
    "\n",
    "# Filter out data from 2022\n",
    "test_results_data = test_results_data[test_results_data.index.year != 2022]\n",
    "\n",
    "# Function to plot metrics\n",
    "def plot_two_metrics(data, metric1, metric2, title, is_annual=False):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(data.index, data[metric1], marker='o', label=metric1)\n",
    "    plt.plot(data.index, data[metric2], marker='o', label=metric2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Year' if is_annual else 'Month')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if is_annual:\n",
    "        plt.xticks(data.index, [str(year.year) for year in data.index])\n",
    "    else:\n",
    "        plt.xticks(pd.date_range(start=data.index.min(), end=data.index.max(), freq='YS'), rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{results_path}/{title}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Monthly plots\n",
    "plot_two_metrics(test_results_data, 'F1-Score', 'Accuracy', 'F1-Score and Accuracy (Monthly)')\n",
    "plot_two_metrics(test_results_data, 'Precision', 'Recall', 'Precision and Recall (Monthly)')\n",
    "\n",
    "# Annual plots\n",
    "annual_metrics_data = test_results_data[['Precision', 'Recall', 'F1-Score', 'Accuracy']].resample('AS').mean()\n",
    "plot_two_metrics(annual_metrics_data, 'F1-Score', 'Accuracy', 'F1-Score and Accuracy (Annual)', is_annual=True)\n",
    "plot_two_metrics(annual_metrics_data, 'Precision', 'Recall', 'Precision and Recall (Annual)', is_annual=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cosine similarity data from a CSV file and set 'Month' as the index column\n",
    "cosine_similarity_df = pd.read_csv(\"cosine_similarity_monthly.csv\", index_col='Month')\n",
    "cosine_similarity_df.index = pd.to_datetime(cosine_similarity_df.index)  # Convert index to datetime\n",
    "\n",
    "cosine_similarity_df = cosine_similarity_df[cosine_similarity_df.index.year != 2022] # Remove 2022 data\n",
    "\n",
    "# Function to plot cosine similarity with correct annual labels\n",
    "def plot_cosine_similarity(data, title, is_annual=False):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(data.index, data['Cosine Similarity'], marker='o', label='Cosine Similarity')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Year' if is_annual else 'Month')\n",
    "    plt.ylabel('Cosine Similarity')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    if is_annual:\n",
    "        # Adjusting the x-ticks for annual data\n",
    "        plt.xticks(data.index, [str(year.year) for year in data.index])\n",
    "    else:\n",
    "        # For monthly data, ensure x-ticks show only the first month of each year\n",
    "        plt.xticks(pd.date_range(start=data.index.min(), end=data.index.max(), freq='YS'), rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{results_path}/{title}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Monthly plot for cosine similarity with annual labels\n",
    "plot_cosine_similarity(cosine_similarity_df, 'Monthly Cosine Similarity')\n",
    "\n",
    "# Annual plot for cosine similarity\n",
    "# Using 'AS' to label based on the start of the year\n",
    "annual_cosine_similarity = cosine_similarity_df.resample('AS').mean()\n",
    "plot_cosine_similarity(annual_cosine_similarity, 'Annual Cosine Similarity', is_annual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapiro-Wilk Test for Normality\n",
    "This test will help determine if the variables have a normal distribution, which is a prerequisite for performing Pearson's correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "# Function to perform Shapiro-Wilk test\n",
    "def shapiro_wilk_test(data, alpha=0.05):\n",
    "    stat, p = shapiro(data)\n",
    "    print('Statistics=%f, p=%f' % (stat, p))\n",
    "    if p > alpha:\n",
    "        print('Sample looks normally distributed')\n",
    "    else:\n",
    "        print('Sample does not look normally distributed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute Shapiro-Wilk test on f1-score and cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if the recall values have a normal distribution\n",
    "print(\"Recall Shapiro-Wilk Test:\")\n",
    "shapiro_wilk_test(recall_data)\n",
    "# Testing if the precision values have a normal distribution\n",
    "print(\"Precision Shapiro-Wilk Test:\")\n",
    "shapiro_wilk_test(precision_data)\n",
    "# Testing if the cosine similarity values have a normal distribution\n",
    "print(\"Cosine Similarity Shapiro-Wilk Test:\")\n",
    "shapiro_wilk_test(cosine_similarity_df['Cosine Similarity'])\n",
    "# Testing if the f1-score values have a normal distribution\n",
    "print(\"F1-Score Shapiro-Wilk Test:\")\n",
    "shapiro_wilk_test(f1_score_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Pearson Correlation Coefficient\n",
    "This section calculates Pearson's correlation coefficient to explore the relationship between the F1 scores and cosine similarity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def pearson(data):\n",
    "    pearson_corr, p_value = pearsonr(data, cosine_similarity_df['Cosine Similarity'])\n",
    "\n",
    "    print(f\"Correlation coefficient: {pearson_corr}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Calculate Pearson's correlation coefficient for precision\n",
    "print(\"Precision Pearson's Correlation:\")\n",
    "pearson(precision_data)\n",
    "# Calculate Pearson's correlation coefficient for recall\n",
    "print(\"Recall Pearson's Correlation:\")\n",
    "pearson(recall_data)\n",
    "# Calculate Pearson's correlation coefficient for f1-score\n",
    "print(\"F1-Score Pearson's Correlation:\")\n",
    "pearson(f1_score_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Spearman Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def spearman(data):\n",
    "    # Calculate Spearman's correlation coefficient between F1 scores and cosine similarity\n",
    "    spearman_corr, spearman_p_value = spearmanr(data, cosine_similarity_df['Cosine Similarity'])\n",
    "\n",
    "    print(f\"Correlation coefficient: {spearman_corr}\")\n",
    "    print(f\"P-value: {spearman_p_value}\")\n",
    "    \n",
    "# Calculate Spearman's correlation coefficient for precision\n",
    "print(\"Precision Spearman's Correlation:\")\n",
    "spearman(precision_data)\n",
    "# Calculate Spearman's correlation coefficient for recall\n",
    "print(\"Recall Spearman's Correlation:\")\n",
    "spearman(recall_data)\n",
    "# Calculate Spearman's correlation coefficient for f1-score\n",
    "print(\"F1-Score Spearman's Correlation:\")\n",
    "spearman(f1_score_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
